{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama PDF RAG Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loaded successfully: ./data/academmic-new.md\n",
      " loaded successfully: ./data/bus-new.md\n",
      " loaded successfully: ./data/cutoof-new.md\n",
      " loaded successfully: ./data/holidays.md\n",
      " loaded successfully: ./data/lab-end.md\n",
      " loaded successfully: ./data/placements.md\n",
      " loaded successfully: ./data/results-new.md\n",
      " loaded successfully: ./data/sem-end-new.md\n",
      " loaded successfully: ./data/syllabus.md\n",
      " loaded successfully: ./data/time_tables.md\n"
     ]
    }
   ],
   "source": [
    "# Load PDF\n",
    "all_documents = []  # Stores all loaded documents\n",
    "for filename in os.listdir(folder_path):\n",
    "    local_path = os.path.join(folder_path,filename)\n",
    "    if local_path:\n",
    "        # loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "        loader = UnstructuredMarkdownLoader(file_path=local_path) \n",
    "        # loader = UnstructuredHTMLLoader(file_path=local_path)  better than markdwon\n",
    "        # loader = UnstructuredFileLoader(file_path=local_path) good\n",
    "        data = loader.load()\n",
    "        all_documents.extend(data)  # Add to combined list\n",
    "        print(f\" loaded successfully: {local_path}\")\n",
    "    else:\n",
    "        print(\"not Upload a  file : {local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 793 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    collection_name=\"local-rag\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM and retrieval\n",
    "#local_model = \"llama3.2\"  # or whichever model you prefer\n",
    "# llm = ChatOllama(model=local_model)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma(\n",
    "    collection_name=\"local-rag\",\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided text, here's a partial list of CSE placements with packages.  The data is incomplete and inconsistently formatted across the documents, making a fully comprehensive answer impossible:\n",
       "\n",
       "\n",
       "* **ARMIET Asst Professor (CSE):** 6.6 Lakhs\n",
       "* **Cognida. ai. Software Engineer:** 7.0 Lakhs (2 placements)\n",
       "* **IBM Software Engineer:** 11.0 Lakhs (1 placement)\n",
       "* **Accolite Software Engineer:** 8.0 Lakhs (2 placements)\n",
       "* **Accenture Adv. Associate Software Engineer:** 6.5 Lakhs (1 placement)\n",
       "* **Accenture Associate Software Engineer:** 4.5 Lakhs (33 placements)\n",
       "* **PayPal PPO Software Engineer:** 34.0 Lakhs (1 placement)\n",
       "* **Microsoft PPO Software Engineer:** 51.0 Lakhs (1 placement)\n",
       "* **ThoughtSpot Software Engineer:** 11.0 Lakhs (6 placements)\n",
       "* **Amazon Software Development Engineer:** 47.0 Lakhs (1 placement)\n",
       "* **Ideagen Trainee Software Engineer:** 7.0 Lakhs (1 placement)\n",
       "* **CloudAngles Software Engineer Trainee:** 5.0 Lakhs (2 placements)\n",
       "\n",
       "\n",
       "Note:  This list only includes information explicitly stating \"CSE\" and a CTC (Cost To Company) figure.  Many other entries mention software engineering roles, but without explicit CSE designation.  Also, some entries are missing crucial details like the number of placements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1\n",
    "chat_with_pdf(\"What are the placements in CSE with packages?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Optional: Clean up when done \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mvector_db\u001b[49m.delete_collection()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVector database deleted successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_db' is not defined"
     ]
    }
   ],
   "source": [
    "# Optional: Clean up when done \n",
    "vector_db.delete_collection()\n",
    "print(\"Vector database deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
